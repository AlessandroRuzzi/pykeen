# paper: https://vixra.org/abs/2112.0095
# reference implementation: https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/master/TripleRE%2BNodepiece/ogb_wikikg2/model.py
metadata:
  title: Learn WikiKG2 Dataset with NodePiece + TripleRE as described by Yu et al., 2022
pipeline:
  dataset: OGBWikiKG2
  dataset_kwargs:
    create_inverse_triples: True
  evaluator: sampled
  evaluator_kwargs:
    filtered: true
    num_negatives: 1000  # TODO: use given set of negatives
  loss: nssa
  model: TripleRE
  model_kwargs:
    embedding_dim: 200   # for both anchors and mid relations, as well as for head/tail relations
    interaction: TripleRE
    aggregation: mlp
    tokenizers: [ Anchor, Relation ]
    num_tokens: [ 20, 12 ]  # 20 anchors, 12 relations
    tokenizers_kwargs:
      - selection: Mixture
        selection_kwargs:
          selections: [ Degree, PageRank, Random ]
          ratios: [ 0.4, 0.4, 0.2 ]
          num_anchors: 20000
      - { } # no params for RelationTokenizer
    # https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L84-L88
    initializer: uniform_
    # (gamma + epsilon) / dim, cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L52-L60
    # gamma = 6 cf. https://raw.githubusercontent.com/LongYu-360/TripleRE-Add-NodePiece/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/run_ogb.sh
    # epsilon = 2 cf. https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L36
    # (6 + 2) / 200 = 8 / 200 = 0.04
    initializer_kwargs:
      a: -0.04
      b: 0.04
    # L2 normalization of entity representations, cf.
    # https://github.com/LongYu-360/TripleRE-Add-NodePiece/blob/994216dcb1d718318384368dd0135477f852c6a4/TripleRE%2BNodepiece/ogb_wikikg2/model.py#L323-L324
    entity_normalizer: normalize
    entity_normalizer_kwargs:
      p: 2
      dim: -1
  optimizer: Adam
  optimizer_kwargs:
    lr: 0.001
  training_kwargs:
    batch_size: 512
    negative_sampler: basic
    negative_sampler_kwargs:
      num_negs_per_pos: 128
    num_epochs: 121
    label_smoothing: 0.3
  training_loop: sLCWA
  stopper: early
  stopper_kwargs:
    metric: inverse_harmonic_mean_rank
results:
  nondeterministic: # https://github.com/snap-stanford/ogb/blob/c8f0d2aca80a4f885bfd6ad5258ecf1c2d0ac2d9/ogb/linkproppred/evaluate.py#L235
    mean_reciprocal_rank: 0.6866
